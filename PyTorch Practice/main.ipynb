{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensor Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "tensor([1, 2])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5)\n",
    "print(x)\n",
    "\n",
    "x = torch.tensor((1,2))\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size():  torch.Size([2, 3])\n",
      "x.shape:  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "print(\"x.size(): \", x.size())\n",
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# check data type\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# requires_grad argument\n",
    "# This will tell pytorch that it will need to calculate the gradients for this tensor\n",
    "# later in your optimization steps\n",
    "# e.g. this is a variable in your model that you want to optimize\n",
    "x = torch.tensor([5.5, 3], requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations with Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.4585, 0.3084, 0.5104],\n",
      "        [0.4450, 0.6249, 0.2298]])\n",
      "tensor([[2.1812, 3.2427, 1.9591],\n",
      "        [2.2473, 1.6001, 4.3510]])\n"
     ]
    }
   ],
   "source": [
    "# Operations\n",
    "x = torch.ones(2, 3)\n",
    "y = torch.rand(2, 3)\n",
    "\n",
    "# addition\n",
    "z = x + y\n",
    "\n",
    "# subtraction\n",
    "z = x - y\n",
    "\n",
    "# multiplication\n",
    "z = x * y\n",
    "\n",
    "# division\n",
    "z = x / y\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6832, 0.0078, 0.5288],\n",
      "        [0.3430, 0.2727, 0.4996],\n",
      "        [0.2094, 0.9740, 0.7433]])\n",
      "x[:, 0] tensor([0.6832, 0.3430, 0.2094])\n",
      "x[0, :] tensor([0.6832, 0.0078, 0.5288])\n",
      "x[0, 0] tensor(0.6832)\n",
      "x[0, 0].item() 0.6832377910614014\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "x = torch.rand(3, 3)\n",
    "print(x)\n",
    "\n",
    "print(\"x[:, 0]\", x[:, 0])\n",
    "print(\"x[0, :]\", x[0, :])\n",
    "print(\"x[0, 0]\", x[0, 0])\n",
    "\n",
    "# Get the actual value if only 1 element in your tensor\n",
    "print(\"x[0, 0].item()\", x[0, 0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8314, 0.6465, 0.3504, 0.5952],\n",
      "        [0.4828, 0.6971, 0.6084, 0.3003],\n",
      "        [0.2078, 0.6992, 0.2212, 0.7253],\n",
      "        [0.0794, 0.7376, 0.3740, 0.3098]])\n",
      "tensor([0.8314, 0.6465, 0.3504, 0.5952, 0.4828, 0.6971, 0.6084, 0.3003, 0.2078,\n",
      "        0.6992, 0.2212, 0.7253, 0.0794, 0.7376, 0.3740, 0.3098])\n",
      "tensor([[0.8314, 0.6465, 0.3504, 0.5952, 0.4828, 0.6971, 0.6084, 0.3003],\n",
      "        [0.2078, 0.6992, 0.2212, 0.7253, 0.0794, 0.7376, 0.3740, 0.3098]])\n"
     ]
    }
   ],
   "source": [
    "# Reshape with torch.view()\n",
    "x = torch.rand(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # if -1 pytorch will automatically determine the necessary size\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n",
    "Converting a Torch Tensor to a NumPy array and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Careful: If the Tensor is on the CPU (not the GPU),\n",
    "# both objects will share the same memory location, so changing one\n",
    "# will also change the other\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# numpy to torch with .from_numpy(x), or torch.tensor() to copy it\n",
    "import numpy as np\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a) # they share the same memory location\n",
    "c = torch.tensor(a)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "\n",
    "# again be careful when modifying\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Support\n",
    "\n",
    "By default all tensors are created on the CPU. But we can also move them to the GPU (if it's available ), or create them directly on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.rand(2, 2).to(device) # move tensors to GPU device if available\n",
    "\n",
    "# it is a small optimizations\n",
    "x = torch.rand(2, 2, device=device)  # or directy create them on GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autograd\n",
    "\n",
    "The autograd package provides automatic differentiation for all operations on Tensors. Generally speaking, torch.autograd is an engine for computing the vector-Jacobian product. It computes partial derivates while applying the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6541, 0.5788, 0.0460], requires_grad=True)\n",
      "tensor([2.6541, 2.5788, 2.0460], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x10f86fb50>\n"
     ]
    }
   ],
   "source": [
    "# requires_grad = True -> tracks all operations on the tensor\n",
    "x = torch.rand(3, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21.1331, 19.9506, 12.5577], grad_fn=<MulBackward0>)\n",
      "tensor(17.8805, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.3082, 5.1576, 4.0919])\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx\n",
    "\n",
    "# !!! Careful!!! backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!! optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop a tensor from tracking history:\n",
    "\n",
    "For example during the training loop when we want to update our weights, or after training during evaluation. These operations should not be part of the gradient computation. To prevent this, we can use:\n",
    "\n",
    "* `x.requires_grad_(False)`\n",
    "* ``x.detach()`\n",
    "* wrap in `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x107f05750>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "a = torch.rand(2, 2)\n",
    "b = (a * a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "b = (a * a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.rand(2, 2, requires_grad=True)\n",
    "b = a.detach()\n",
    "\n",
    "print(a.requires_grad)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.rand(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    b = a ** 2\n",
    "    print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Autograd\n",
    "Linear Regression example:\n",
    "\n",
    "$f(x) = w * x + b$\n",
    "\n",
    "here : `f(x) = 2 * x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "# f = w * x  + b\n",
    "# here : f = 2 * x\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8, 10, 12, 14, 16], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(12.0) = 0.000\n",
      "epoch 10: w = 1.998, loss = 0.000\n",
      "epoch 20: w = 2.000, loss = 0.000\n",
      "epoch 30: w = 2.000, loss = 0.000\n",
      "epoch 40: w = 2.000, loss = 0.000\n",
      "epoch 50: w = 2.000, loss = 0.000\n",
      "epoch 60: w = 2.000, loss = 0.000\n",
      "epoch 70: w = 2.000, loss = 0.000\n",
      "epoch 80: w = 2.000, loss = 0.000\n",
      "epoch 90: w = 2.000, loss = 0.000\n",
      "epoch 100: w = 2.000, loss = 0.000\n",
      "Prediction after training: f(12.0) = 24.000\n"
     ]
    }
   ],
   "source": [
    "X_test = 12.0\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "print(f'Prediction before training: f({X_test}) = {forward(X_test).item():.3f}')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(x)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # calculate gradients = backward pass dl/dw\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "      w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.3f}')\n",
    "\n",
    "print(f'Prediction after training: f({X_test}) = {forward(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model, Loss & Optimizer\n",
    "\n",
    "A typical PyTorch pipeline looks like this:\n",
    "\n",
    "1. Design model (input, output, forward pass with different layers)\n",
    "2. Construct loss and optimizer\n",
    "3. Training loop:\n",
    "  - Forward = compute prediction and loss\n",
    "  - Backward = compute gradients\n",
    "  - Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples = 8, n_features = 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x  + b\n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples, SHAPE is important\n",
    "x = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8], [10], [12], [14], [16]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(f\"n_samples = {n_samples}, n_features = {n_features}\")\n",
    "\n",
    "# 0) Create a TEST sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5.0) = 2.579\n",
      "epoch 10: w = 1.977, loss = 0.003\n",
      "epoch 20: w = 1.979, loss = 0.003\n",
      "epoch 30: w = 1.980, loss = 0.003\n",
      "epoch 40: w = 1.981, loss = 0.002\n",
      "epoch 50: w = 1.981, loss = 0.002\n",
      "epoch 60: w = 1.982, loss = 0.002\n",
      "epoch 70: w = 1.983, loss = 0.002\n",
      "epoch 80: w = 1.984, loss = 0.002\n",
      "epoch 90: w = 1.984, loss = 0.002\n",
      "epoch 100: w = 1.985, loss = 0.002\n",
      "Prediction after training: f(5.0) = 10.009\n"
     ]
    }
   ],
   "source": [
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # define different layers\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input_size = output_size = n_features\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f({X_test.item()}) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(epochs):\n",
    "    # predict = forward pass with our model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y_pred, y)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        w, b = model.parameters() # unpack parameters\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l.item():.3f}')\n",
    "\n",
    "print(f'Prediction after training: f({X_test.item()}) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Network\n",
    "GPU, Datasets, DataLoader, Transforms, Neural Network, Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
